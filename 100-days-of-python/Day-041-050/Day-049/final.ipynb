{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A Chat bot using Ollama (Complete offline)\n",
    "\n",
    "So, here I am trying to create a RAG Q&A Bot which will be able to get context data from a web URL and then use that information as context to answer any question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will try to first load the data from a web URL using the Langchain Document loaders -> WebBasedLoader\n",
    "\n",
    "Right now, I am loading my blog article on Franken PHP and trying to ask questions on that blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.amitavroy.com/articles/beyond-boundaries-how-frankenphp-redefines-php-application-runtimes-2024-01-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we loaded the URL using WebBaseLoader and then got the entire document in the \"docs\" variable using the loader.load method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the FAISS vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Prompt set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, FrankenPHP can speed things up in several ways:\n",
      "\n",
      "1. Early hint support: FrankenPHP claims a whopping 30% reduction in loading time by harnessing early hint support, as mentioned in the context.\n",
      "2. Worker Script Mode: By using worker mode, FrankenPHP can power up performance by running entirely from memory, providing a significant boost for high-traffic scenarios.\n",
      "3. Prometheus metrics and tracing: With built-in support for Prometheus metrics and tracing, FrankenPHP can help keep an eye on things and monitor the application's performance in real-time.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can FrankenPHP speed things up?\"\n",
    "response = retrieval_chain.invoke({\"input\": question})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the answer to your question is yes, you can run a Laravel binary using FrankenPHP. The context mentions that FrankenPHP supports running PHP applications as a standalone binary, which implies that it can handle various PHP versions and frameworks, including Laravel.\n",
      "\n",
      "FrankenPHP is designed to be a modern PHP app server that offers numerous advantages over traditional PHP approaches. It uses Go's goroutine features and relies on Caddy server for HTTPS support in local development. FrankenPHP also supports HTTP/1.1, HTTP/2, and HTTP/3 protocols, which can help reduce round-trip times and improve performance.\n",
      "\n",
      "Since Laravel is a PHP framework, it should be compatible with FrankenPHP's standalone binary approach. Therefore, you can create a Laravel binary using the FrankenPHP build tool and run it as a standalone application. This could simplify your deployment process and reduce the attack surface area by using fewer software packages in production.\n",
      "\n",
      "However, it is essential to note that FrankenPHP is still a relatively new project, and there may be some limitations or unresolved issues when running complex PHP applications like Laravel. As such, you should thoroughly test your Laravel application with FrankenPHP before deploying it to production to ensure optimal performance and stability.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can I run a Laravel binary and run it?\"\n",
    "response = retrieval_chain.invoke({\"input\": question})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
