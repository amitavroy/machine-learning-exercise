{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Preprocess PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import pickle\n",
    "from transformers import RobertaTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Open the PDF file\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        # Extract text from each page\n",
    "        text = ' '.join(page.extract_text() for page in pdf_reader.pages)\n",
    "        # Clean up the text (remove extra spaces, newlines, etc.)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './../../data/sample.pdf'\n",
    "pdf_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk, Tokenize, and Save to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def chunk_text(text, chunk_size):\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "chunk_size = 1000\n",
    "tokenized_chunks = []\n",
    "\n",
    "# Tokenize each chunk and append to the list\n",
    "for chunk in chunk_text(pdf_text, chunk_size):\n",
    "    tokenized_chunk = tokenizer(chunk)\n",
    "    tokenized_chunks.append(tokenized_chunk)\n",
    "\n",
    "# Save the list of tokenized chunks to a pickle file\n",
    "with open(\"tokenized_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenized_chunks, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load file and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForQuestionAnswering: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main topic of the PDF?\n",
      "Answer: Nick Carter,\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from transformers import RobertaTokenizer, pipeline\n",
    "\n",
    "# Load tokenized chunks from pickle file\n",
    "with open(\"tokenized_chunks.pkl\", \"rb\") as f:\n",
    "    tokenized_chunks = pickle.load(f)\n",
    "\n",
    "# Combine tokenized chunks into a single list of tokens\n",
    "flat_tokenized_text = [token for chunk in tokenized_chunks for token in chunk[\"input_ids\"]]\n",
    "\n",
    "# Initialize the tokenizer and question-answering pipeline\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "nlp = pipeline(\"question-answering\", model=model_name, tokenizer=tokenizer)\n",
    "\n",
    "# Decode the combined tokenized text into a single string\n",
    "flat_text = tokenizer.decode(flat_tokenized_text)\n",
    "\n",
    "# Example question\n",
    "question = \"What is the main topic of the PDF?\"\n",
    "\n",
    "# Use the question-answering pipeline to answer the question\n",
    "QA_input = {\"question\": question, \"context\": flat_text}\n",
    "answer = nlp(QA_input)[\"answer\"]\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
